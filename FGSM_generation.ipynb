{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running on new images\n",
    "This notebook will walk you step by step through the process of using a pre-trained model to detect traffic signs in an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# tf.enable_eager_execution()\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "matplotlib.use('TkAgg')  # Use TkAgg to show images\n",
    "\n",
    "# from PIL import Image\n",
    "import glob as glob\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "\n",
    "%matplotlib inline\n",
    "# tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sys.path.append('../models/research')  # Replace with the path to TensorFlow Object Detection API\n",
    "sys.path.append('../darkflow')  # Replace with the path to Darkflow\n",
    "\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Object Detection API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME = 'faster_rcnn_inception_resnet_v2_atrous'\n",
    "# MODEL_NAME = 'faster_rcnn_resnet_101'\n",
    "# MODEL_NAME = 'faster_rcnn_resnet50'\n",
    "MODEL_NAME = 'faster_rcnn_inception_v2'\n",
    "# MODEL_NAME = 'rfcn_resnet101'\n",
    "# MODEL_NAME = 'ssd_inception_v2'\n",
    "# MODEL_NAME = 'ssd_mobilenet_v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to frozen detection graph. This is the actual model that is used for the traffic sign detection.\n",
    "MODEL_PATH = os.path.join('models', MODEL_NAME)\n",
    "PATH_TO_CKPT = os.path.join(MODEL_PATH,'inference_graph/frozen_inference_graph.pb')\n",
    "\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = os.path.join('gtsdb_data', 'gtsdb3_label_map.pbtxt')\n",
    "\n",
    "NUM_CLASSES = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a (frozen) Tensorflow model into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "    od_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading label map\n",
    "Label maps map indices to category names, so that when our convolution network predicts `2`, we know that this corresponds to `mandatory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yuxing/my_project/envpy3.6/lib/python3.6/site-packages/object_detection/utils/label_map_util.py:132: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "item {\n",
      "  name: \"prohibitory\"\n",
      "  id: 1\n",
      "}\n",
      "item {\n",
      "  name: \"mandatory\"\n",
      "  id: 2\n",
      "}\n",
      "item {\n",
      "  name: \"danger\"\n",
      "  id: 3\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PATH_TO_LABELS = '/home/yuxing/my_project/traffic-sign-detection/gtsdb_data/gtsdb3_label_map.pbtxt'\n",
    "\n",
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)\n",
    "print(label_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(image):\n",
    "    (im_width, im_height) = image.size\n",
    "    #return np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(np.uint8)\n",
    "    return np.array(image.getdata()).reshape((im_height, im_width, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PATH_TO_TEST_IMAGES_DIR = '/home/yuxing/my_project/traffic-sign-detection/test_images'\n",
    "\n",
    "TEST_IMAGE_PATHS = glob.glob(os.path.join(PATH_TO_TEST_IMAGES_DIR, '*.jpg'))\n",
    "\n",
    "# Size, in inches, of the output images.\n",
    "IMAGE_SIZE = (20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to print detections\n",
    "def print_detections(sess, image_np_expanded, detection_graph, confidence_threshold=0.5):\n",
    "    # Get the tensors for detection\n",
    "    image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "    detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "    detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "    detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "\n",
    "    # Run the session to get the detection boxes, scores, and classes\n",
    "    (boxes, scores, classes) = sess.run(\n",
    "        [detection_boxes, detection_scores, detection_classes],\n",
    "        feed_dict={image_tensor: image_np_expanded}\n",
    "    )\n",
    "\n",
    "    # Filter out the results with confidence scores less than the threshold\n",
    "    filter_indices = np.where(scores[0] >= confidence_threshold)[0]\n",
    "\n",
    "    # Apply the filter to boxes, scores and classes\n",
    "    filtered_boxes = boxes[0][filter_indices]\n",
    "    filtered_scores = scores[0][filter_indices]\n",
    "    filtered_classes = classes[0][filter_indices]\n",
    "\n",
    "    print(\"Filtered Detection Boxes:\", filtered_boxes)\n",
    "    print(\"Filtered Detection Scores:\", filtered_scores)\n",
    "    print(\"Filtered Detection Classes:\", filtered_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Detection Boxes: [[0.35613373 0.45915684 0.42445537 0.51562685]]\n",
      "Filtered Detection Scores: [0.9997478]\n",
      "Filtered Detection Classes: [3.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "saved_detection_results = {}\n",
    "\n",
    "# Run inference and visualization\n",
    "with detection_graph.as_default():\n",
    "    with tf.Session(graph=detection_graph) as sess:\n",
    "        for idx, image_path in enumerate(TEST_IMAGE_PATHS):\n",
    "            image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "            boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "            scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "            classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "            num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "\n",
    "            image_data = tf.read_file(image_path)\n",
    "            image_decoded = tf.image.decode_image(image_data)\n",
    "            image_np = sess.run(image_decoded)\n",
    "            image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "\n",
    "            (boxes, scores, classes, num_detections) = sess.run(\n",
    "                [boxes, scores, classes, num_detections],\n",
    "                feed_dict={image_tensor: image_np_expanded})\n",
    "\n",
    "            saved_detection_results[image_path] = {\n",
    "                'boxes': boxes,\n",
    "                'scores': scores,\n",
    "                'classes': classes,\n",
    "                'num_detections': num_detections\n",
    "            }\n",
    "\n",
    "            # Print and visualize detection results on adversarial image\n",
    "            print_detections(sess, image_np_expanded, detection_graph)\n",
    "            \n",
    "            vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "                image_np,\n",
    "                np.squeeze(boxes),\n",
    "                np.squeeze(classes).astype(np.int32),\n",
    "                np.squeeze(scores),\n",
    "                category_index,\n",
    "                use_normalized_coordinates=True,\n",
    "                line_thickness=6)\n",
    "            plt.figure(idx, figsize=IMAGE_SIZE)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(image_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approx_logits 8.56662580498044\n",
      "original_loss 8.56662580498044\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 36, 18, 2) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 83, 206, 1) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 23, 87, 1) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 159, 250, 0) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 151, 40, 1) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 164, 85, 0) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 190, 245, 0) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 42, 191, 0) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 150, 4, 1) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 16, 2, 2) : 0\n",
      "approx_logits 8.553555286885475\n",
      "perturbed_loss 8.553555286885475\n",
      "Gradient at idx (0, 52, 134, 2) : 0\n",
      "approx_logits 8.568507040973765\n",
      "perturbed_loss 8.568507040973765\n",
      "Gradient at idx (0, 80, 85, 1) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 110, 1, 2) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 57, 202, 0) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 106, 3, 1) : 0\n",
      "approx_logits 8.602988645970571\n",
      "perturbed_loss 8.602988645970571\n",
      "Gradient at idx (0, 77, 155, 0) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 176, 190, 2) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 84, 17, 0) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 12, 16, 0) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 46, 18, 0) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 148, 78, 1) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 29, 14, 0) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 162, 197, 2) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 18, 107, 0) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 182, 58, 1) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 172, 249, 0) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 9, 97, 0) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 158, 212, 0) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 37, 203, 0) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 136, 8, 1) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 36, 234, 1) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 167, 226, 2) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 180, 96, 0) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 80, 184, 1) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 126, 63, 0) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 135, 19, 2) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 132, 242, 1) : 0\n",
      "approx_logits 8.57039182133771\n",
      "perturbed_loss 8.57039182133771\n",
      "Gradient at idx (0, 93, 95, 2) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 55, 231, 1) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 9, 102, 2) : 0\n",
      "approx_logits 8.565999511651597\n",
      "perturbed_loss 8.565999511651597\n",
      "Gradient at idx (0, 110, 79, 2) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 8, 10, 1) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 126, 205, 2) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 26, 244, 0) : 0\n",
      "approx_logits 8.567252490648952\n",
      "perturbed_loss 8.567252490648952\n",
      "Gradient at idx (0, 91, 74, 2) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 147, 39, 2) : 0\n",
      "approx_logits 8.567252490648952\n",
      "perturbed_loss 8.567252490648952\n",
      "Gradient at idx (0, 39, 171, 1) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 110, 230, 1) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 93, 198, 1) : 0\n",
      "approx_logits 8.56662580498044\n",
      "perturbed_loss 8.56662580498044\n",
      "Gradient at idx (0, 29, 236, 1) : 0\n",
      "Perturbed Loss: 8.56662580498044\n",
      "Original Loss: 8.56662580498044\n",
      "grad_approx [[[[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]]]\n",
      "grad_approx [[[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxing/my_project/envpy3.6/lib/python3.6/site-packages/ipykernel_launcher.py:90: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Detection Boxes: [[0.35621923 0.45944858 0.42426622 0.51550686]]\n",
      "Filtered Detection Scores: [0.99970526]\n",
      "Filtered Detection Classes: [3.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxing/my_project/envpy3.6/lib/python3.6/site-packages/ipykernel_launcher.py:123: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Create save directory for images\n",
    "SAVE_DIR = 'saved_images'\n",
    "if not os.path.exists(SAVE_DIR):\n",
    "    os.makedirs(SAVE_DIR)\n",
    "\n",
    "# Function to downsample an image\n",
    "def downsample(image, factor=0.2):\n",
    "    return cv2.resize(image, None, fx=factor, fy=factor, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "# Function to upsample an image\n",
    "def upsample(image, original_shape):\n",
    "    return cv2.resize(image, (original_shape[1], original_shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "def compute_loss(sess, image_np_expanded, detection_graph):\n",
    "    image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "    scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "    classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "    \n",
    "    (scores, classes) = sess.run([scores, classes], feed_dict={image_tensor: image_np_expanded})\n",
    "    max_score_index = np.argmax(scores)\n",
    "    score = scores[0][max_score_index]\n",
    "    approx_logits = np.log(score / (1 - score))\n",
    "    print('approx_logits',approx_logits)\n",
    "    return approx_logits\n",
    "\n",
    "def approximate_gradient(sess, image_np_expanded, detection_graph, h=300, num_samples=50):\n",
    "    original_loss = compute_loss(sess, image_np_expanded, detection_graph)\n",
    "    print('original_loss',original_loss)\n",
    "    grad_approx = np.zeros_like(image_np_expanded)\n",
    "    \n",
    "    indices = np.random.choice(np.prod(image_np_expanded.shape), num_samples, replace=False)\n",
    "    \n",
    "    for flat_idx in indices:\n",
    "        idx = np.unravel_index(flat_idx, image_np_expanded.shape)\n",
    "        perturb = np.zeros_like(image_np_expanded)\n",
    "        perturb[idx] = h\n",
    "        perturbed_image = image_np_expanded + perturb\n",
    "        perturbed_loss = compute_loss(sess, perturbed_image, detection_graph)\n",
    "        print('perturbed_loss',perturbed_loss)\n",
    "        grad_approx[idx] = (perturbed_loss - original_loss) / h\n",
    "        print(\"Gradient at idx\", idx, \":\", grad_approx[idx])\n",
    "\n",
    "\n",
    "    print(\"Perturbed Loss:\", perturbed_loss)\n",
    "    print(\"Original Loss:\", original_loss)\n",
    "    print('grad_approx',grad_approx)\n",
    "    \n",
    "    return grad_approx\n",
    "\n",
    "\n",
    "# Main execution\n",
    "with detection_graph.as_default():\n",
    "    with tf.Session(graph=detection_graph) as sess:\n",
    "        # Define tensors for detection\n",
    "        image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "        boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "        scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "        classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "        num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "\n",
    "        for idx, image_path in enumerate(TEST_IMAGE_PATHS):\n",
    "            image_data = tf.read_file(image_path)\n",
    "            image_decoded = tf.image.decode_image(image_data)\n",
    "            image_np = sess.run(image_decoded)\n",
    "            original_shape = image_np.shape\n",
    "            \n",
    "            # Downsample image for approximating gradient\n",
    "            image_np_low_res = downsample(image_np)\n",
    "            image_np_low_res_expanded = np.expand_dims(image_np_low_res, axis=0)\n",
    "            \n",
    "            # Compute the approximate gradient on low-res image\n",
    "            grad_approx_low_res = approximate_gradient(sess, image_np_low_res_expanded, detection_graph)\n",
    "            \n",
    "            # Upsample gradient to original resolution\n",
    "            grad_approx = upsample(grad_approx_low_res[0], original_shape)\n",
    "\n",
    "            print('grad_approx',grad_approx)\n",
    "\n",
    "            # Create adversarial image\n",
    "            eps = 100\n",
    "            adv_image = image_np + eps * np.sign(grad_approx)\n",
    "            adv_image = np.clip(adv_image, 0, 255).astype(np.uint8)\n",
    "            \n",
    "            # Display the adversarial image\n",
    "            plt.figure(idx, figsize=IMAGE_SIZE)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(adv_image)\n",
    "            plt.show()\n",
    "\n",
    "            # Save adversarial image\n",
    "            save_path = os.path.join(SAVE_DIR, f'adv_image_{idx}.jpg')\n",
    "            cv2.imwrite(save_path, cv2.cvtColor(adv_image, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "            # Reload the saved adversarial image\n",
    "            adv_image_data = cv2.imread(save_path)\n",
    "            adv_image_rgb = cv2.cvtColor(adv_image_data, cv2.COLOR_BGR2RGB)\n",
    "            adv_image_expanded = np.expand_dims(adv_image_rgb, axis=0)\n",
    "\n",
    "            (boxes, scores, classes, num_detections) = sess.run(\n",
    "                [boxes, scores, classes, num_detections],\n",
    "                feed_dict={image_tensor: adv_image_expanded}\n",
    "            )\n",
    "\n",
    "            # Print and visualize detection results on adversarial image\n",
    "            print_detections(sess, adv_image_expanded, detection_graph)\n",
    "            \n",
    "            vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "                adv_image_rgb,\n",
    "                np.squeeze(boxes),\n",
    "                np.squeeze(classes).astype(np.int32),\n",
    "                np.squeeze(scores),\n",
    "                category_index,\n",
    "                use_normalized_coordinates=True,\n",
    "                line_thickness=6,\n",
    "                min_score_thresh=0.5\n",
    "            )\n",
    "            \n",
    "            plt.figure(idx, figsize=IMAGE_SIZE)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(adv_image_rgb)\n",
    "            plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
